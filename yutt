def adjust_learning_rate(optimizer, epoch, lr=0.1, lr_decay_ratio=0.1, epoch_drop=(), writer=None):
    """Simple learning rate drop according to the provided parameters"""
    optim_factor = 0
    for i, ep in enumerate(epoch_drop):
        if epoch > ep:
            optim_factor = i + 1
    lr = lr * lr_decay_ratio ** optim_factor

    # log to TensorBoard
    if writer is not None:
        writer.add_scalar('learning_rate', lr, epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
